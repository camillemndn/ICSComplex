# ICS for complex data {#sec-icsfree}

A naive approach to ICS for complex data would be to apply multivariate
ICS to coordinate vectors in a basis. This not only ignores the metric
on the space when the basis is not orthonormal, but also gives a
potentially different ICS method for each choice of basis [as in
@archimbaud_ics_2022]. Defining a unique coordinate-free ICS problem
avoids defining multiple ICS methods and having to discuss the potential
links between them, thus making our approach more intrinsic. In
particular, it leads to more interpretable invariant components that are
of the same nature as the considered complex random objects. In the case
of functional or distributional data, the usual framework assumes that
the data objects reside in an infinite-dimensional Hilbert space, which
leads to non-orthonormal bases and incomplete inner product spaces. We
choose to restrict our attention to finite-dimensional approximations of
the data in the framework of Euclidean spaces, which are particularly
suitable here because ICS is known to fail when the dimension is larger
than the sample size [@tyler_note_2010]. This suggests that an ICS
method for infinite-dimensional Hilbert spaces would require modifying
the core of the method, which is beyond the scope of this work.

## A coordinate-free ICS problem {#sec-icsproblem}

In order to generalise invariant coordinate selection
[@tyler_invariant_2009 def. 1] to a coordinate-free framework in a
Euclidean space $E$, we need to eliminate any reference to a coordinate
system, which means replacing coordinate vectors by abstract vectors,
matrices by linear mappings, bases or quadratic forms, depending on the
context. This coordinate emancipation procedure will ensure that our
definition of ICS for an $E$-valued random object $X$ does not depend on
any particular choice of basis of $E$ to represent $X$.

Following this methodology, we are able to immediately generalise the
definition of (affine equivariant) scatter operators from random vectors
in $E = \mathbb R^p$ [as defined in @tyler_invariant_2009 eq. 3] to
random objects in a Hilbert space $E$. This is a perfect example of how
the coordinate-free framework can be used to extend existing work to
infinite-dimensional spaces. For further details, see @def-scatter in the
Appendix. A notable difference from [@tyler_invariant_2009] is that we
work directly with random objects instead of their underlying
distributions. In particular, we introduce an affine invariant space
$\mathcal E$ of random objects on which the scatter operators are
defined and to which we assume that $X$ belongs. For example,
$\mathcal E = L^p (\Omega, E)$ corresponds to assuming the existence of
the $p$ first moments of $\| X \|$.

Again, emancipating from coordinates allows us to naturally generalise
ICS to complex random objects in a Euclidean space.

::: {#def-ics}
# Coordinate-free ICS
Let $(E, \langle \cdot, \cdot \rangle)$ be a Euclidean space of dimension
$p$, $\mathcal E \subseteq L^1 (\Omega, E)$ an affine invariant set of
integrable $E$-valued random objects, $S_1$ and $S_2$ two scatter
operators on $\mathcal E$ and $X \in \mathcal E$. The invariant
coordinate selection problem $\operatorname{ICS} (X,{S_1},{S_2})$ is to
find a basis ${H} = (h_1, \dots, h_p)$ of $E$ and a finite
non-increasing real sequence
$\Lambda = (\lambda_1 \geq \ldots \geq \lambda_p)$ such that
$$
\operatorname{ICS} (X,{S_1},{S_2}) : \left\{ \begin{array}{lcl}
\langle{S_1} [X] h_j, h_{j'} \rangle &=& \delta_{jj'} \\
\langle{S_2} [X] h_j, h_{j'} \rangle &=& \delta_{jj'} \lambda_j
\end{array} \text{ for all } 1 \leq j,j' \leq p, \right.
$${#eq-icsdef} where
$\delta_{jj'}$ equals $1$ if $j=j'$ and $0$ otherwise. Such a basis $H$
is called an $\operatorname{ICS} (X,{S_1},{S_2})$ eigenbasis, whose
elements are $\operatorname{ICS} (X,{S_1},{S_2})$ eigenobjects. Such a
$\Lambda$ is called an $\operatorname{ICS} (X,{S_1},{S_2})$ spectrum,
whose elements are called $\operatorname{ICS} (X,{S_1},{S_2})$
eigenvalues or generalised kurtosis. Given an
$\operatorname{ICS} (X,{S_1},{S_2})$ eigenbasis $H$ and
$1 \leq j \leq p$, the real number
$$z_j = \langle X - \mathbb EX, h_j \rangle$${#eq-ic} is called the $j$-th
invariant coordinate (in the eigenbasis $H$).
:::

In @def-ics, our coordinate emancipation procedure does not yield a generalisation to
infinite-dimensional Hilbert spaces, where a basis $H$ would not be
properly defined as it is not necessarily orthonormal.

::: {.remark}
# Multivariate case
If $E = \mathbb R^p$, we identify
$S_1$ and $S_2$ with their associated $(p\times p)$-matrices in the
canonical basis, and we identify an ICS eigenbasis $H$ with the
$(p\times p)$-matrix of its vectors stacked column-wise, so that we
retrieve the classical formulation of invariant coordinate selection by
@tyler_invariant_2009.
:::

In the ICS problem @eq-icsdef, the scatter operators $S_1$ and $S_2$ do not
play symmetrical roles. This is because the usual method of solving
$\operatorname{ICS} (X,{S_1},{S_2})$ is to use the associated inner
product of $S_1 [X]$, which requires $S_1 [X]$ to be injective. In that
case, @prp-existence in the Appendix proves the existence of
solutions to the ICS problem.

Another way to understand the coordinate-free nature of this ICS problem
is to work with data isometrically represented in two spaces and to
understand how we can relate a given ICS problem in the first space to a
corresponding ICS problem in the second. This is the object of the
following proposition, which will be used in @sec-icscoord.

::: {#prp-isometry}
Let $\varphi : (E, \langle \cdot, \cdot \rangle_E) \rightarrow (F, \langle \cdot, \cdot \rangle_F)$
be an isometry between two Euclidean spaces of dimension $p$,
$\mathcal E \subseteq L^1 (\Omega, E)$ an affine invariant set of
integrable $E$-valued random objects, $S_1^{\mathcal E}$ and
$S_2^{\mathcal E}$ two affine equivariant scatter operators on
$\mathcal E$. Then:

(a) $\mathcal F = \varphi (\mathcal E) = \{ \varphi (X^{\mathcal E}), X^{\mathcal E} \in \mathcal E \}$
    is an affine invariant set of integrable $F$-valued random objects,
    and we denote
    $X^{\mathcal F} = \varphi(X^{\mathcal E}) \in \mathcal F$ whenever
    $X^{\mathcal E} \in \mathcal E$;

(b) $S_\ell^{\mathcal F} :  X^{\mathcal F} \in \mathcal F \mapsto \varphi \circ S_\ell^{\mathcal E} [X^{\mathcal E}] \circ \varphi^{-1}, \ell \in \{1,2\},$
    are two affine equivariant scatter operators on $\mathcal F$;

(c) $H^{\mathcal F} = \varphi(H^{\mathcal E}) = (\varphi (h_1^{\mathcal E}), \dots, \varphi (h_p^{\mathcal E}))$
    is a basis of $F$ whenever
    $H^{\mathcal E}  = (h_1^{\mathcal E}, \dots, h_p^{\mathcal E})$ is a
    basis of $E$.

For any $E$-valued random object $X^{\mathcal E} \in \mathcal E$, any
basis $H^{\mathcal E} = (h_1^{\mathcal E}, \dots, h_p^{\mathcal E})$ of
$E$, and any finite non-increasing real sequence
$\Lambda = (\lambda_1 \geq \ldots \geq \lambda_p)$ the following
assertions are equivalent:

(i) $(H^{\mathcal E}, \Lambda)$ solves
    $\operatorname{ICS} (X^{\mathcal E}, S_1^{\mathcal E}, S_2^{\mathcal E})$
    in the space $E$

(ii) $(H^{\mathcal F}, \Lambda)$ solves
     $\operatorname{ICS} (X^{\mathcal F}, S_1^{\mathcal F}, S_2^{\mathcal F})$
     in the space $F$.
:::

## The case of weighted covariance operators {#sec-weighted}

A difficulty in ICS is to find interesting scatter operators that
capture the non-ellipticity of the random object. Usually, for
multivariate data, we use the pair of scatter matrices
$(\operatorname{Cov}, \operatorname{Cov}_4)$. In this section, we define
an important family of scatter operators, namely the weighted covariance
operators, which contains both $\operatorname{Cov}$ and
$\operatorname{Cov}_4$. They are explicitly defined by coordinate-free
formulas which allow us to relate ICS problems using weighted covariance
operators between any two Euclidean spaces. We denote by
$\mathcal{GL} (E)$ the group of linear automorphisms of $E$ and by
$A^{1/2}$ the unique non-negative square root of a linear mapping $A$.

::: {#def-covw}
# Weighted covariance operators
For any measurable function $w: \mathbb R^+ \rightarrow \mathbb R$, let
$$\mathcal E_w = \left\{ X \in L^2 (\Omega, E) \left| \, \operatorname{Cov}[X] \in \mathcal{GL}(E)
\text{ and } w \left( \left\| \operatorname{Cov}[X]^{-1/2} (X - \mathbb EX) \right\| \right) \| X - \mathbb EX \| \in L^2 (\Omega, \mathbb R) \right. \right\}.$$
Note that $\mathcal E_w$ is an affine invariant set of integrable
$E$-valued random objects. For $X \in \mathcal E_w$, we define the
$w$-weighted covariance operator $\operatorname{Cov}_w [X]$ by
$$\forall (x,y) \in E^2, \langle \operatorname{Cov}_w [X] x, y \rangle = \mathbb E \left[ w^2 \left( \left\| \operatorname{Cov}[X]^{-1/2} (X - \mathbb EX) \right\| \right) \langle X - \mathbb EX, x \rangle \langle X - \mathbb EX, y \rangle \right].$${#eq-covw}
When necessary, we will also write $\operatorname{Cov}_w^E$ for the
$w$-weighted covariance operator on $E$ to avoid any ambiguity. It is
easy to check that weighted covariance operators are affine equivariant
scatter operators in the sense of @def-scatter.
:::

::: {#exm-cov}
If $w=1$, we retrieve $\operatorname{Cov}$, the usual
covariance operator on $L^2 (\Omega, E)$.
:::

::: {#exm-cov4}
If for $x \in \mathbb R^+$, $w(x) = (p+2)^{-1/2} x$, we
obtain the fourth-order moment operator $\operatorname{Cov}_4$ [as in
@nordhausen_usage_2022 for the case $E = \mathbb R^p$] on
$\mathcal E_w = \left\{ X \in L^4 (\Omega, E) \, | \, \operatorname{Cov}[X] \in \mathcal{GL} (E) \right\}$.
:::

The following corollary applies @prp-isometry to the
pair of $w_\ell$-weighted covariance operators
$S_\ell^{\mathcal E} = \operatorname{Cov}_{w_\ell}, \ell \in  \{ 1, 2 \}$,
for which the corresponding $S_\ell^{\mathcal F}$ are exactly the
$w_\ell$-weighted covariance operators on $F$.

::: {#cor-isometry-covw}
Let $(E, \langle \cdot, \cdot \rangle_E) \overset{\varphi}{\rightarrow} (F, \langle \cdot, \cdot \rangle_F)$
be an isometry between two Euclidean spaces of dimension $p$ and
$w_1, w_2 : \mathbb R^+ \rightarrow \mathbb R$ two measurable functions.
For any integrable $E$-valued random object
$X \in \mathcal E_{w_1} \cap \mathcal E_{w_2}$ (with the notations from
@def-covw), the equality
$$\operatorname{Cov}_{w_\ell}^F [\varphi(X)] = \varphi \circ \operatorname{Cov}_{w_\ell}^E [X] \circ \varphi^{-1}$${#eq-isometry-covw}
holds for $\ell \in  \{ 1, 2 \}$, as well as the equivalence between the
following assertions, for any basis $H = (h_1, \dots, h_p)$ of $E$, and
any finite non-increasing real sequence
$\Lambda = (\lambda_1 \geq \ldots \geq \lambda_p)$:

(i) $(H, \Lambda)$ solves
    $\operatorname{ICS} (X, \operatorname{Cov}_{w_1}^E, \operatorname{Cov}_{w_2}^E)$
    in the space $E$.

(ii) $(\varphi(H), \Lambda)$ solves
     $\operatorname{ICS} (\varphi(X), \operatorname{Cov}_{w_1}^F, \operatorname{Cov}_{w_2}^F)$
     in the space $F$.
:::

## Implementation {#sec-icscoord}

In order to implement coordinate-free ICS in any Euclidean space $E$, we
restrict our attention to the pair
$(\operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$ of weighted
covariance operators defined in @sec-weighted.
Note that we could also transport other known scatter matrices, such as
the Minimum Covariance Determinant [defined in
@rousseeuw_multivariate_1985], back to the space $E$ using @prp-isometry, but
this approach would no longer be coordinate-free.

We now choose a basis $B=(b_1, \dots, b_p)$ of $E$ in order to represent
each element $x$ of $E$ by its coordinate vector
$[x]_B = ([x]_{b_1} \dots [x]_{b_p})^\top \in \mathbb R^p$. Then, the
following corollary of @prp-isometry allows
one to relate the coordinate-free approach in $E$ to three different
multivariate approaches applied to the coordinate vectors in any basis
$B$ of $E$, where the Gram matrix
$G_B = (\langle b_j, b_{j'} \rangle)_{1 \leq j,j' \leq p}$ appears,
accounting for the non-orthonormality of $B$. Notice that, since the ICS
problem has been defined in @sec-icsproblem
without any reference to a particular basis, it is obvious that the
basis $B$ has no influence on ICS.

::: {#cor-coord}
Let $(E, \langle \cdot, \cdot \rangle)$ be a Euclidean
space of dimension $p$, $w_1, w_2 : \mathbb R^+ \rightarrow \mathbb R$
two measurable functions. Let $B$ be any basis of $E$,
$G_B = (\langle b_j, b_{j'} \rangle)_{1 \leq j,j' \leq p}$ its Gram
matrix and $[ \cdot ]_B$ the linear map giving the coordinates in $B$.
For any $X \in \mathcal E_{w_1} \cap \mathcal E_{w_2}$ (with the
notations from @def-covw) = (h_1, \dots, h_p$ of $E$, and
any finite non-increasing real sequence
$\Lambda = (\lambda_1 \geq \ldots \geq \lambda_p)$ the following
assertions are equivalent:

(1) $(H, \Lambda)$ solves
    $\operatorname{ICS} (X, \operatorname{Cov}_{w_1}^E, \operatorname{Cov}_{w_2}^E)$
    in the space $E$

(2) $({G_B^{1/2}} [H]_B, \Lambda)$ solves
    $\operatorname{ICS} ({G_B^{1/2}} [X]_B, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathbb R^p$

(3) [$([H]_B, \Lambda)$ solves
    $\operatorname{ICS} ({G_B} [X]_B, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathbb R^p$]{#eq-3}

(4) $({G_B} [H]_B, \Lambda)$ solves
    $\operatorname{ICS} ([X]_B, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathbb R^p$

*where $[H]_B$ denotes the non-singular $p \times p$ matrix representing
the basis $([h_1]_B, \dots, [h_p]_B)$ of $\mathbb R^p$.*
:::

In practice, we prefer @eq-3 (first
transforming the data by the Gram matrix of the basis) because it is the
only one that does not require inverting the Gram matrix in order to
recover the eigenobjects. Then, the problem is reduced to multivariate
ICS, already implemented in the R package `ICS` using the QR
decomposition [@archimbaud_numerical_2023]. This QR approach enhances
stability compared to methods based on a joint diagonalisation of two
scatter matrices, which can be numerically unstable in some
ill-conditioned situations.

After we obtain the ICS eigenelements, we can use them to reconstruct
the original random object, in order to interpret the contribution of
each invariant component. @prp-reconstruction in the Appendix generalises the
multivariate reconstruction formula to complex data. In order to
implement this reconstruction, we need the coordinates of the elements
of the dual ICS eigenbasis. Identifying the basis $[H]_B$ with the
matrix whose columns are its vectors, the dual basis $[H^*]_B$ is the
matrix $$[H^*]_B = \left( [H]_B^\top G_B \right)^{-1}.$$

::: {.remark}
# Empirical ICS and estimation
In order to work with
samples of complex random objects, we can study the particular case of a
finite $E$-valued random object $X$ where we have a fixed sample
$D_n=(x_1, \dots, x_n)$ and we assume that $X$ follows the empirical
probability distribution $P_{D_n}$ of $(x_1, \dots, x_n)$. In that case,
the expressions (in @def-covw) for instance) of the form $\mathbb E f(X$ for any
function $f$ are discrete and equal to $\frac1n \sum_{i=1}^n f(x_i)$.

Now, let us assume that we observe an i.i.d. sample
$D_n = (X_1, \dots, X_n)$ following the distribution of an unknown
$E$-valued random object $X_0$. We can estimate solutions of the problem
$\operatorname{ICS} (X_0, S_1, S_2)$ from @def-ics by working
conditionally on the data $(X_1, \dots, X_n)$ and taking the particular
case where $X$ follows the empirical probability distribution $P_{D_n}$.
This defines estimates of the $\operatorname{ICS} (X_0, S_1, S_2)$
eigenobjects as solutions of an ICS problem involving empirical scatter
operators. Since the population version of ICS for a complex random
object $X \in E$ is more concise than its sample counterpart for
$D_n = (X_1, \dots, X_n)$, we shall use the notations of the former in
the next sections.
:::

## ICS for compositional data {#sec-icscoda}

The specific case of coordinate-free ICS for compositional data is
equivalent to the approach of @ruiz-gazen_detecting_2023. To see this,
let us consider the simplex
$E = (\mathcal S^{p+1}, \oplus, \odot, \langle \cdot, \cdot \rangle_{\mathcal S^{p+1}})$
of dimension $p$ in $\mathbb R^{p+1}$ with the Aitchison structure
[@pawlowskyglahn_modeling_2015]. The results from 5.1
(resp. 5.2) in [@ruiz-gazen_detecting_2023] can be recovered by
applying @cor-isometry-covw to any isometric log-ratio transformation
[see @pawlowskyglahn_modeling_2015 for a definition] (resp. the centred
log-ratio transformation).

@cor-coord gives a new characterisation of the problem
$\operatorname{ICS} (X, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
using additive log-ratio transformations. For a given index
$1 \leq j \leq p$, let $B_j = (b_1, \dots, b_p)$ denote the basis of
$\mathcal S^{p+1}$ corresponding to the $\operatorname{alr}_j$
transformation, i.e. obtained by taking the canonical basis of
$\mathbb R^{p+1}$, removing the $j$-th vector and applying the
exponential. In that case, it is easy to compute the $p \times p$ Gram
matrix of $B_j$:
$$G_{B_j} = I_p - \frac1{p+1} \mathbf 1_p \mathbf 1_p ^\top = \begin{pmatrix}
        1 - \frac1{p+1} & -\frac1{p+1} & \dots & -\frac1{p+1} \\
        -\frac1{p+1} & \ddots & \ddots & \vdots \\
        \vdots & \ddots & \ddots & - \frac1{p+1} \\
        -\frac1{p+1} & \dots & -\frac1{p+1} & 1 - \frac1{p+1}
    \end{pmatrix}.$$ Then, we get the equivalence between the following
two ICS problems:

1.  $(H, \Lambda)$ solves
    $\operatorname{ICS} (X, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathcal S^{p+1}$

2.  $(\operatorname{alr}_j (H), \Lambda)$ solves
    $\operatorname{ICS} (\operatorname{clr} (X)^{(j)}, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathbb R^p$

where $\operatorname{clr} (x)^{(j)} = G_{B_j} \operatorname{alr}_j (x)$
is the centred log-ratio transform of $x \in \mathcal S^{p+1}$ from
which the $j$-th coordinate has been removed. This suggests a new and
fastest implementation of invariant coordinate selection for
compositional data, in an unconstrained space and only requiring the
choice of an index $j$ instead of a full contrast matrix.

## ICS for functional data {#sec-icsfun}

The difficulty of functional data (in the broader sense, encompassing
density data) is twofold: first, functions are usually analysed within
the infinite-dimensional Hilbert space $L^2 (a,b)$, second, a random
function is almost never observed for every argument, but rather on a
discrete grid. This grid can be regular or irregular, deterministic or
random, dense (the grid step goes to zero) or sparse. We describe a
general framework for adapting coordinate-free ICS to functional data,
solving both difficulties at the same time by smoothing the observed
values into a random function $u$ that belongs to a Euclidean subspace
$E$ of $L^2 (a,b)$.

### Choosing an approximating Euclidean subspace

We usually choose polynomial spaces, spline spaces with given knots and
order, or spaces spanned by a truncated Hilbert basis of $L^2 (a,b)$. In
practice, this choice also depends on the preprocessing method that we
have in mind to smooth discrete observations into functions.

### Preprocessing the observations into the approximating space

Considering a dense, deterministic grid $(t_1, \dots, t_N)$, we need to
reconstruct an $E$-valued random function $u$ from its noisy observed
values $(u(t_1) + \varepsilon_1, \dots, u(t_N) + \varepsilon_N)$. There
are many well-documented approximation techniques to carry out this
preprocessing step, such as interpolation, spline smoothing, or Fourier
methods [for a detailed presentation, see @eubank_nonparametric_2014].

### Solving ICS in the approximating space

Once we have obtained an $E$-valued random function $u$, we can apply
the method described in @sec-icscoord to reduce
$\operatorname{ICS} (u, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
to a multivariate problem on the coordinates in a basis of $E$. In
particular, for an orthonormal basis $B$ of $E$ (such as a Fourier basis
or a Hermite polynomial basis), @cor-coord gives the
equivalence between the following two assertions:

1.  $(H, \Lambda)$ solves
    $\operatorname{ICS} (u, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $E$

2.  $([H]_B, \Lambda)$ solves
    $\operatorname{ICS} ([u]_B, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $\mathbb R^p$.

If $E$ is a finite-dimensional spline space, we usually work with the
coordinates of $u$ in a B-spline basis of $E$, but then we should take
into account its Gram matrix, as in @cor-coord.\
ICS has previously been defined for multivariate functional data by
@archimbaud_ics_2022, who define a pointwise method and a global method.
Unlike the pointwise approach, which is specific to multivariate
functional data, the global method can also be applied to univariate
functional data in $L^2 (a,b)$, as it corresponds to applying
multivariate ICS to truncated coordinate vectors in a Hilbert basis of
$L^2 (a,b)$. The above framework retrieves the global method in
[@archimbaud_ics_2022] as a particular case when taking a Hilbert basis
$B$ of $L^2 (a,b)$ and solving coordinate-free ICS in the space $E$
spanned by the $p$ first elements of $B$.

## ICS for distributional data {#sec-icsdens}

A first option to adapt ICS to density data would be to consider it as
constrained functional data and directly follow the approach of @sec-icsfun.
However, distributional data does not reduce to density data [such as
absorbance spectra studied in @ferraty_functional_2002], as it can also
be histogram data or sample data (such as the dataset of temperature
samples analysed in @sec-appliout). Moreover, the framework of Bayes Hilbert
spaces, described by [@van_den_boogaart_bayes_2014] and recalled in the
Appendix, is specifically adapted to the study of distributional data.
Taking into account the infinite-dimensional nature of distributional
data, we follow a similar framework as the one of Section
@sec-icsfun,
restricting our attention to finite-dimensional subspaces $E$ of the
Bayes space $B^2(a,b)$ with the Lebesgue measure as reference.

### Choosing an approximating Euclidean space

Following smoothing splines methods, adapted to Bayes spaces by
@machalova_preprocessing_2016 and recalled in the Appendix, we choose to
work in the space $E = \mathcal C^{\Delta \gamma}_d (a,b)$ of
compositional splines on $(a,b)$ of order $d+1$ with knots
$\Delta \gamma = (\gamma_1, \dots, \gamma_k)$. Note that the centred
log-ratio transform $\operatorname{clr}$ is an isometry between $E$ and
the space $F = \mathcal Z^{\Delta \gamma}_d (a,b)$ of zero-integral
splines on $(a,b)$ of order $d+1$ and with knots
$\Delta \gamma = (\gamma_1, \dots, \gamma_k)$. They both have dimension
$p = k + d$.

### Preprocessing the observations into the approximating space {#sec-preproc}

We consider the special cases of histogram data and of sample data. In
the former, we follow [@machalova_compositional_2021] to smooth each
histogram into a compositional spline in $E$. In the latter, we assume
that a random density is observed through a finite random sample
$(X_1, \dots, X_N)$ drawn from it. The preprocessing step consists in
estimating the density from the observed sample. To perform the
estimation, we need a nonparametric estimation procedure that yields a
compositional spline belonging to $E$. That is why we opt for maximum
penalised likelihood (MPL) density estimation, introduced by
@silverman_estimation_1982. The principle of MPL is to maximise a
penalised version of the log-likelihood over an infinite-dimensional
space of densities without parametric assumptions. The penalty is the
product of a smoothing parameter $\lambda$ by the integral over the
interval of interest of the square of the $m$-th derivative of the log
density. Therefore, the objective functional is a functional of the log
density. Due to the infinite dimension of the ambient space, the
likelihood term alone is unbounded above, hence the penalty term is
necessary. In our case of densities on an interval $(a,b)$, we select
the value $m=3$ so that [according to @silverman_estimation_1982 Theorem
2.1] when the smoothing parameter tends to infinity, the estimated
density converges to the parametric maximum likelihood estimate in the
exponential family of densities whose logarithm is a polynomial of
degree less than or equal to 2. This family comprises the uniform
density, exponential and Gaussian densities truncated to $(a,b)$. In
order to use MPL in $B^2(a,b)$, we need to add extra smoothness
conditions and therefore we restrict attention to the densities of
$B^2(a,b)$ whose log belongs to the Sobolev space of order $m$ on
$(a,b)$, thus ensuring the existence of the penalty term. Note that
compositional splines verify these conditions. With Theorem 4.1 in
[@silverman_estimation_1982], the optimisation problem has at least a
solution. Since the estimate $f$ of the density of $(X_1, \dots, X_N)$
needs to belong to the chosen finite-dimensional subspace
$E = \mathcal C^{\Delta \gamma}_d (a,b)$, we restrict MPL to $E$, using
the R function `fda::density.fd`, designed by @ramsay_fda_2024. This
function returns the coordinates of $\log (f)$ in the B-spline basis
with knots $\Delta \gamma$ and order $d+1$, that we project onto
$\mathcal Z^{\Delta \gamma}_d (a,b)$ and to which we apply
$\operatorname{clr}^{-1}$ so that we obtain an element of
$\mathcal C^{\Delta \gamma}_d (a,b)$.

### Solving ICS in the approximating space

We have now obtained an $E$-valued random compositional spline $f$. In
order to work with two weighted covariance operators
$\operatorname{Cov}_{w_1}$ and $\operatorname{Cov}_{w_2}$, where
$w_1, w_2: \mathbb R^+ \rightarrow \mathbb R$ are two measurable
functions, we assume that
$f \in \mathcal E_{w_1} \cap \mathcal E_{w_2}$, using the notations of
@def-covw.
Now, we refer to @sec-icscoord to reduce the problem
$\operatorname{ICS} (f, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
to a multivariate ICS problem on the coordinates of $f$ in the CB-spline
basis of $\mathcal C^{\Delta \gamma}_d (a,b)$ [defined in
@machalova_compositional_2021], transformed by the Gram matrix of said
CB-spline basis. Note that @cor-isometry-covw applied to the centred log-ratio isometry
between $\mathcal C^{\Delta \gamma}_d (a,b)$ and
$\mathcal Z^{\Delta \gamma}_d (a,b)$ gives the equivalence between:

1.  $(H, \Lambda)$ solves
    $\operatorname{ICS} (f, \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $E = \mathcal C^{\Delta \gamma}_d (a,b)$

2.  $(\operatorname{clr} (H), \Lambda)$ solves
    $\operatorname{ICS} (\operatorname{clr} (f), \operatorname{Cov}_{w_1}, \operatorname{Cov}_{w_2})$
    in the space $F = \mathcal Z^{\Delta \gamma}_d (a,b)$.

Then, it is completely equivalent, and useful for implementation, to
work with the coordinates of $\operatorname{clr} (f)$ in the ZB-spline
basis of $\mathcal Z^{\Delta \gamma}_d (a,b)$.
