```{r setup}
#| include: false

source("setup.R")
set.seed(0)
```

# Outlier detection for complex data using ICS {#sec-icscomplexout}

## Implementation of ICS on complex data for outlier detection {#sec-icsout}

We propose using ICS to detect outliers in complex data, specifically in
scenarios with a small proportion of outliers (typically 1 to 2%). For
this, we follow the three-step procedure defined by
@archimbaud_ics_2018, modifying the first step based on the
implementation of coordinate-free ICS in @sec-icscoord.

### Computing the invariant coordinates

For the scatter operators, we follow the recommendation of
@archimbaud_ics_2018 who compare several pairs of more or less robust
scatter estimators in the context of a small proportion of outliers, and
conclude that $(\operatorname{Cov}, \operatorname{Cov}_4)$ is the best
choice. Thus, we use the empirical scatter pair
$(\operatorname{Cov}, \operatorname{Cov}_4)$ (see @exm-cov and @exm-cov4),
and compute the eigenvalues $\lambda_1 \geq \ldots \geq \lambda_p$, and the
invariant coordinates $z_{ji}, 1 \leq j \leq p$, for each observation
$X_i, 1 \leq i \leq n$. As outlined in @sec-icscoord, for
a given sample of random complex objects $D_n= \{X_1, \dots, X_n\}$ in
a Euclidean space $E$, solving the empirical version of ICS is
equivalent to solving an ICS problem in a multivariate framework [see
@tyler_invariant_2009] with the coordinates of the objects in a basis
$B$ of $E$. In order to choose a basis, we follow the specific
recommendations for each type of data from @sec-icscoda and
@sec-icsdens.

### Selecting the invariant components

The second step of the outlier detection procedure based on ICS is the
selection of the $\kappa<p$ relevant invariant components and the
computation of the ICS distances. For each of the $n$ observations, the
ICS distance is equal to the Euclidean norm of the reconstructed data
using the $\kappa$ selected invariant components. In the case of a small
proportion of outliers and for the scatter pair
$(\operatorname{Cov}, \operatorname{Cov}_4)$, the invariant components
of interest are associated with the largest eigenvalues and the squared
ICS distances are equal to $\displaystyle \sum_{j=1}^\kappa z_{ji}^2$.
As noted by @archimbaud_ics_2018, there exist several methods for the
selection of the number of invariant components. One approach is to
examine the scree plot, as in PCA. This method, recommended by
@archimbaud_ics_2018, is not automatic. Alternative automatic selection
methods apply univariate normality tests to each component, starting
with the first one, and using some Bonferroni correction [for further
details see page 13 of @archimbaud_ics_2018]. In the present paper, we
use the scree plot approach when there is no need of an automatic
method, and we use the D'Agostino normality test for automatic
selection. The level for the first test (before Bonferroni correction)
is 5%. Dimension reduction involves retaining only the first $\kappa$
components of ICS instead of the original $p$ variables. Note that when
all the invariant components are retained, the ICS distance is equal to
the Mahalanobis distance.

### Choosing a cut-off

The computation of ICS distances allows to rank the observations in
decreasing order, with those having the largest distances potentially
being outliers. However, in order to identify the outlying densities, we
need to define a cut-off, and this constitutes the last step of the
procedure. Following @archimbaud_ics_2018, we derive cut-offs based on
Monte Carlo simulations from the standard Gaussian distribution. For a
given sample size and number of variables, we generate 10,000 standard
Gaussian samples and compute the empirical quantile of order 97.5% of
the ICS-distances using the three steps previously described. An
observation with an ICS distance larger than this quantile is flagged as
an outlier.\
The procedure described above has been illustrated in several examples
[see @archimbaud_ics_2018], and is implemented in the R package
`ICSOutlier` [see @nordhausen_icsoutlier_2023]. However, in the context
of densities, the impact of preprocessing parameters (see @sec-icsdens) on
the ICSOutlier procedure emerges as a crucial question that needs to be
examined.

## Influence of the preprocessing parameters for the density data application {#sec-toyex}

As a toy example, consider the densities of the maximum daily
temperatures for the 26 provinces of the two regions Red River Delta and
Northern Midlands and Mountains in Northern Vietnam between 2013 and
2016. We augment this data set made of 104 densities by adding the
provinces AN GIANG and BAC LIEU from Southern Vietnam in the same time
period. The total number of observations is thus 112. Details on the
original data and their source are provided in @sec-datades.

```{r fig-vnt_north_south_map}
#| fig-cap: Map of Vietnam showing the 63 provinces, with the three regions under
#|   study colour-coded. The 28 provinces included in the toy example are labelled.

library(dda)
library(tidyverse)
library(sf)

data(vietnam_provinces)
data(vietnam_regions)

vietnam_provinces |>
  mutate(
    province = if_else(region %in% c("NMM", "RRD") | province %in% c("ANGIANG", "BACLIEU"), province, NA),
    region = if_else(region %in% c("NMM", "RRD", "MDR"), region, NA)
  ) |>
  left_join(vietnam_regions, by = c("region" = "code")) |>
  select(!region) |>
  rename(region = name) |>
  ggplot(aes(fill = region, label = province)) +
  geom_sf() +
  geom_sf_label(fill = "white", size = 3) +
  scale_fill_discrete(na.translate = FALSE) +
  scale_x_continuous(breaks = seq(from = -180, to = 180, by = 2)) +
  labs(x = "longitude", y = "latitude")
```

```{r fig-vnt_north_south_densclr}
#| fig-cap: Plots of the 112 densities (left panel) and clr densities
#|   (right panel), colour-coded by region for the toy example.
#| fig-subcap: ""
#| fig-width: 7.5
#| fig-height: 5
#| layout-ncol: 2

# Filter the data set
data(vietnam_temperature)
selected_years <- 2013:2016

vnt <- vietnam_temperature |>
  filter(year %in% selected_years) |>
  filter(region %in% c("NMM", "RRD") | province %in% c("ANGIANG", "BACLIEU")) |>
  left_join(vietnam_regions, by = c("region" = "code")) |>
  select(!region) |>
  rename(region = name) |>
  arrange(region, province)

vnts <- vnt |>
  mutate(t_max = as_dd(t_max,
    lambda = 1, nbasis = 12, mc.cores = 12
  ))

vnts |>
  plot_funs(t_max, color = region) +
  theme_legend_inside +
  labs(x = "temperature (deg. Celsius)", y = "density")

vnts |>
  mutate(t_max = as.fd(center(t_max))) |>
  plot_funs(t_max, color = region) +
  theme(legend.position = "none") +
  labs(x = "temperature (deg. Celsius)", y = "centred clr(density)")
```

@fig-vnt_north_south_map displays a map of Vietnam with the
contours of all provinces and coloured according to their administrative
region, allowing the reader to locate the 26 provinces in the North and
the two in the South. As shown on the left panel of
@fig-vnt_north_south_densclr, the eight densities of the two
provinces from the South for the four years exhibit a very different
shape (in red) compared to the northern provinces (in blue and green),
with much more concentrated maximum temperatures. These two provinces
should be detected as outliers when applying the ICSOutlier methodology.
However, the results may vary depending on the choice of preprocessing
parameters (see @sec-preproc). Our goal is to analyse how the detected
outliers vary depending on the preprocessing when using the maximum
penalised likelihood method with splines of degree less than or equal to
$d=4$. Specifically, we study the influence on the results of ICSOutlier
of the smoothing parameter $\lambda$, the number of inside knots $k$,
and the location of the knots defining the spline basis.

The number $\kappa$ of selected invariant components is fixed at four in
all experiments to facilitate interpretation. This value has been chosen
after viewing the scree plots of the ICS eigenvalues following the
recommendations in @sec-icsout. For each of the experimental scenarios
detailed below, we compute the squared ICS distances of the 112
observations as defined in @sec-icsout,
using $\kappa=4$. Observations are classified as outliers when their
squared ICS distance exceeds the threshold defined in @sec-icsout,
using a level of $2.5\%$. For each experiment, we plot on @fig-vnt_north_south_grid the
indices of the observations from 1 to 112 on the $y$-axis, marking
outlying observations with dark squares. The eight densities from
Southern Vietnam are in red and correspond to indices 1 to 8. We
consider the following scenarios:

-   the knots are either located at the quantiles of the temperature
    values (top panel on @fig-vnt_north_south_grid) or equally spaced
    (bottom panel on @fig-vnt_north_south_grid),
-   from the left to the right of @fig-vnt_north_south_grid, the
    number of knots varies from 0 to 14 by increments of 2, and then
    takes the values 25 and 35 (overall 10 different values). Note that
    when increasing the number of knots beyond 35, the code returns more
    and more errors due to multicollinearity issues and the results are
    not reported.
-   the base-10 logarithm of the parameter $\lambda$ varies from -8 to 8
    with an increment of 1 on the $x$-axis of each plot.

Altogether we have $2\times 10\times 17=340$ scenarios.
@fig-vnt_north_south_grid_summary is a
bar plot showing the observations indices on the $x$-axis and the
frequency of outlier detection across scenarios on the $y$-axis
color-coded by region. The eight densities from the two southern
provinces (AN GIANG and BAC LIEU) across the four years are most
frequently detected as outliers, along with the province of LAI CHAU
(indices 33 to 36), which is located in a mountainous region in
northwest of Vietnam. On the original data, we can see that the LAI CHAU
province corresponds to densities with low values for high maximum
temperatures (above 35Â°C) coupled with relatively high density values
for maximum temperatures below 35Â°C. A few other observations are
detected several times as outliers, but less frequently: indices 53
(TUYEN QUANG in 2013), 96 (QUANG NINH in 2016), and 107 (THANH HOA in
2015).

Looking at @fig-vnt_north_south_grid, we examine the impact of the preprocessing
parameters on the detection of outlying observations. First, note that
the ICS algorithm returns an error when the $\lambda$ parameter is large
(shown as white bands in some plots). This is due to a multicollinearity
problem. Even though the QR version of the ICS algorithm is quite
stable, it may still encounter problems when multicollinearity is
severe. Indeed, when $\lambda$ is large, the estimated densities
converge to densities whose logarithm is a polynomial of degree less
than or equal to 2 (see details in @sec-preproc), and belongs to a 3-dimensional affine
subspace of the Bayes space, potentially with a dimension smaller than
that of the approximating spline space. If we compare the top and the
bottom plots, we do not observe large differences in the outlying
pattern, except for a few observations rarely detected as outliers.
Thus, the knot location has a rather small impact on the ICS results for
this data set. Regarding the impact of the $\lambda$ parameter, the
outlier pattern remains relatively stable when the number of knots is
small (less than or equal to 6), especially when looking at the
densities from the south of Vietnam in red. For a large number of knots,
the observations detected as outliers vary with $\lambda$. The number of
knots has more impact than their location or the $\lambda$ parameter.
When the number of knots is smaller than or equal to 6 (corresponding to
$p=10$ variables), the plots are very similar. However, as $p$
increases, some observations from Southern Vietnam are not detected for
all $\lambda$ values, while another density (QUANG NINH in 2016) is
detected for large $\lambda$ values with equally spaced knots, and to a
lesser extent for knots at temperature quantiles. In
[@archimbaud_ics_2022], ICS is applied to multivariate functional data
with B-splines preprocessing. Based on their empirical experience, the
authors recommend using a dimension $p$ (in their case, the number of
functional components times the number of B-splines coefficients) no
larger than the number of observations divided by 10. Typically in
multivariate analysis, the rule of thumb is that the dimension should
not exceed the number of observations divided by 5. For functional or
distributional data, it appears that even more observations per variable
are needed. The reason for this is not entirely clear, but in the case
of ICS, we can suspect that the presence of multicollinearity, even
approximate, degrades the results. By increasing the number of knots, we
precisely increase the multicollinearity problem, especially for large
values of $\lambda$.

```{r fig-vnt_north_south_grid}
#| fig-cap: Outlier detection by ICS across smoothing parameters for the Vietnam
#|   toy example. *Top:* knots at quantiles; *Bottom:* equally spaced knots.
#|   *$y$-axis:* observation indices; *$x$-axis:* $\lambda$ parameter.
#|   Columns correspond to knot numbers (0-35). Outliers are dark and
#|   colour-coded by region.

smooth_ICS <- function(vnt, knots_pos, nknots, lambda, index) {
  vnts <- vnt |>
    mutate(t_max = as_dd(t_max,
      lambda = lambda, nbasis = 5 + nknots,
      mc.cores = parallel::detectCores() - 1, knots_pos = knots_pos
    ))
  knots_pos_str <- ifelse(knots_pos == "quantiles",
    "Knots at quantiles", "Equally spaced knots"
  )
  index_str <- ifelse(is.na(index), "Auto selection of components",
    paste("# of components:", index)
  )

  print(paste(
    "# of knots:", nknots, "|",
    "log10(lambda):", log10(lambda), "|",
    knots_pos_str, "|",
    index_str
  ))

  if (is.na(index)) {
    list(
      dd = vnts$t_max,
      ics = tryCatch(ICS(as.fd(vnts$t_max)), error = function(e) e),
      ics_out = tryCatch(ICS_outlier(as.fd(vnts$t_max),
        n_cores = parallel::detectCores() - 1
      ), error = function(e) e)
    )
  } else {
    list(
      dd = vnts$t_max,
      ics = tryCatch(ICS(as.fd(vnts$t_max)), error = function(e) e),
      ics_out = tryCatch(ICS_outlier(as.fd(vnts$t_max),
        n_cores = parallel::detectCores() - 1, index = seq_len(index)
      ), error = function(e) e)
    )
  }
}

knots_pos_seq <- c("quantiles", "equally spaced")
nknots_seq <- c(5:19, seq(20, 40, 10)) - 5
lambda_seq <- 10^(-8:8)
index_seq <- 4

param_grid <- expand.grid(
  knots_pos = knots_pos_seq,
  nknots = nknots_seq,
  lambda = lambda_seq,
  index = index_seq
) |>
  arrange(nknots, knots_pos, lambda)

if (!file.exists("data/vnt_north_south_outliers.RData") && !file.exists("data/vnt_north_south.RData")) {
  system.time(
    ics_grid <- param_grid |>
      mutate(id = seq_len(n())) |>
      rowwise() |>
      mutate(res = list(smooth_ICS(vnt, knots_pos, nknots, lambda, index)))
  )
  save(ics_grid, file = "data/vnt_north_south.RData")
}

if (!file.exists("data/vnt_north_south_outliers.RData")) {
  load("data/vnt_north_south.RData")

  ics_grid_out <- ics_grid |>
    rowwise() |>
    filter(!(any(class(res$ics_out) == "error"))) |>
    mutate(res = list(data.frame(
      obs_id = seq_len(length((res$ics_out$outliers))),
      outlying = res$ics_out$outliers == 1,
      distance = res$ics_out$ics_distances
    ))) |>
    unnest(res)

  save(ics_grid_out, file = "data/vnt_north_south_outliers.RData")
}

load("data/vnt_north_south_outliers.RData")

ics_grid_out |>
  left_join(vnt |> mutate(obs_id = row_number()), by = c("obs_id")) |>
  filter(nknots %% 2 == 0 | nknots > 20) |>
  mutate(region = str_wrap(region, 25)) |>
  ggplot(aes(log10(lambda), obs_id, fill = region, alpha = outlying)) +
  geom_tile() +
  facet_grid(cols = vars(nknots), rows = vars(knots_pos)) +
  labs(y = "observation index")
```

```{r fig-vnt_north_south_grid_summary}
#| fig-cap: Frequency of outlier detection by ICS across 340 scenarios with
#|   varying smoothing parameters, for each observation in the Vietnam toy
#|   example.

ics_grid_summary <- ics_grid_out |>
  mutate(outlying = ifelse(outlying, 1, 0)) |>
  group_by(obs_id) |>
  summarise(n_outlying = sum(outlying), mean_distance = mean(distance)) |>
  left_join(vnt |> mutate(obs_id = row_number()), by = c("obs_id")) |>
  select(obs_id, region, province, year, n_outlying, mean_distance)

ics_grid_summary |>
  ggplot(aes(obs_id, n_outlying, fill = region)) +
  geom_bar(stat = "identity") +
  theme_legend_inside_right +
  labs(x = "observation index", y = "outlier detection frequency")
```

From this experimentation, we recommend using knots located at the
quantiles of the measured variable, and a number of knots such that the
number of observations is around 10 times the dimension $p$ (here: the
dimension of the B-spline basis). The base-10 logarithm of parameter
$\lambda$ can be chosen between -2 and 2 to avoid extreme cases and
multicollinearity problems. Moreover, the idea of launching ICS multiple
times with different preprocessing parameter values to confirm an
observation's atypical nature by its repeated detection is a strategy we
retain for real applications, as detailed in @sec-app_smooth.

## Comparison with other methods

We now compare ICS for functional data (presented in @sec-icsfun) to
eight outlier detection methods already existing in the literature, such
as median-based approaches [@murph_visualisation_2024], the modified
band depth method [@sun_functional_2011] and MUOD indices
[@ojo_detecting_2022].

```{r fig-comparison_schemes, message = FALSE}
#| fig-cap: Generation schemes for density data with outliers
#| fig-height: 5

library(tf)

schemes <- c("GP_clr", "Gumbel", "GP_margin")

simulation_gp_clr <- function(n, p) {
  sim <- fdaoutlier::simulation_model4(n, p, mu = 15, outlier_rate = 0.02)
  y <- exp(tfd(sim$data, arg = seq(0, 1, length.out = p)))

  list(
    data = as.matrix(y / tf_integrate(y)),
    true_outliers = replace(integer(n), sim$true_outliers, 1)
  )
}

dgumbel <- function(x, mu = 0, beta = 1) {
  z <- (x - mu) / beta
  exp(-z - exp(-z)) / beta
}

simulation_gumbel <- function(n, p, outlier_rate = 0.02, deterministic = TRUE) {
  tt <- seq(0, 1, length.out = p)
  a <- matrix(rnorm(n, sd = 0.01), nrow = p, ncol = n, byrow = TRUE)
  b <- matrix(rnorm(n, sd = 0.01), nrow = p, ncol = n, byrow = TRUE)
  y <- dgumbel(tt, mu = 0.25 + a, beta = 0.05 + b)
  dtt <- fdaoutlier:::determine(deterministic, n, outlier_rate)
  n_outliers <- dtt$n_outliers
  true_outliers <- dtt$true_outliers
  if (n_outliers > 0) {
    a <- matrix(rnorm(n_outliers, sd = 0.01),
      nrow = p, ncol = n_outliers, byrow = TRUE
    )
    b <- matrix(rnorm(n_outliers, sd = 0.01),
      nrow = p, ncol = n_outliers, byrow = TRUE
    )
    y[, true_outliers] <- dgumbel(tt, mu = 0.32 + a, beta = 0.05 + b)
  }
  rownames(y) <- tt
  return(list(
    data = t(y),
    true_outliers = replace(integer(n), true_outliers, 1)
  ))
}

simulation_gp_margin <- function(n, p) {
  sim <- fdaoutlier::simulation_model5(n, p, outlier_rate = 0.02)
  y <- tibble(f = split(sim$data, row(sim$data))) |>
    mutate(f = as_dd(f, lambda = 1, nbasis = 10, mc.cores = 14))
  tt <- seq(min(sim$data), max(sim$data), length.out = p)
  densities_matrix <- t(eval(c(y$f), tt))
  colnames(densities_matrix) <- tt
  list(
    data = densities_matrix,
    true_outliers = replace(integer(n), sim$true_outliers, 1)
  )
}

generate_data <- function(scheme, n = 200, p = 100) {
  print(paste("Generating", scheme, "data"))
  switch(scheme,
    "GP_clr" = simulation_gp_clr(n, p),
    "Gumbel" = simulation_gumbel(n, p),
    "GP_margin" = simulation_gp_margin(n, p)
  )
}
```

Our simulation uses three density-generating processes with $2\%$ of
outliers. The scheme named `GP_clr`, based on model 4 of the
`fdaoutlier` package [@ojo_detecting_2022 section 4.1], first simulates
a discretised random function in $L^2(0,1)$ from a mixture of two
Gaussian processes with different means, and applies the inverse
$\operatorname{clr}$ transformation to obtain a random density in the
Bayes space $B^2 (0,1)$. The scheme named `GP_margin` first simulates a
discretised random function in $L^2 (0,1)$ using model 5 of the
`fdaoutlier` package, which consists in a mixture of two Gaussian
processes with different covariance operators. Then, the random density
is obtained as a kind of marginal distribution of the discrete values of
the random function, where the $x$-axis is discarded: theses values are
considered as a random sample and smoothed using MPL (see @sec-icsdens with
parameters $\lambda = 1$, $10$ basis functions and knots (as well as
interval bounds) at quantiles of the full sample. This scheme is similar
to the data generating process of the Vietnamese climate dataset.
Finally, the `Gumbel` scheme first draws parameters from a mixture of
two Gaussian distributions in $\mathbb R^2$ and computes the Gumbel
density functions corresponding to these parameters (it generates shift
outliers as described in [@murph_visualisation_2024]). Note that the
output of all the schemes is a set of discretised densities on a regular
grid of size $p=100$ that covers an interval $(a,b)$ (which is $(0,1)$
for `GP_clr` and `Gumbel` and the range of the full sample for
`GP_margin`). In each sample, there are $n=200$ densities.

For the outlier detection methods, we denote them as
`<Approach>_<Metric>` so that for instance, `ICS_B2` refers to ICS for
density data in the Bayes space $B^2 (a,b)$. The steps of the `ICS_B2`
method are as follows. After applying the discrete clr transformation to
each discretised density function, we approximate the underlying clr
transformed smooth density by a smoothing spline in $L^2_0 (a,b)$ using
the preprocessing described in [@machalova_preprocessing_2016]. During
this process, densities should not take values too close to $0$ to avoid
diverging clr, so we replace by $10^{-8}$ all density values below this
threshold. The parameters of the compositional spline spaces are chosen
by the function `fda.usc::fdata2fd`. Then, we solve ICS in the chosen
compositional spline space, automatically selecting the components with
tests as before. The `ICS_L2` method first smooths each discretised
density using splines in $L^2 (a,b)$ treating the densities as ordinary
functional parameters. In the second step, we apply ICS in the chosen
spline space, selecting the components automatically through D'Agostino
normality tests. The MBD [@lopez-pintado_concept_2009] and MUOD
[@azcorra_unsupervised_2018] approaches are implemented using the
package `fdaoutlier` [@ojo_fdaoutlier_2023], either directly
(`<Approach>_L2`) or after transforming the densities into log quantile
densities (`<Approach>_LQD`) or into quantile functions
(`<Approach>_QF`). The median-based methods such as `Median_LQD` and
`Median_Wasserstein` are described in [@murph_visualisation_2024] and
implemented in the `DeBoinR` package from [@murph_deboinr_2023] using
the recommended default parameters.

For each combination between a generating scheme and a method, we
average the TPR (True Positive Rate, or sensitivity) and the FPR (False
Positive Rate, one minus specificity) over $N=200$ repetitions, for each
value of PP (the number of predicted positive) which scales from $0$ to
$n$. We also compute point-wise confidence bounds using the standard
deviation of the TPR over the $N$ repetitions and the standard Gaussian
quantile of order 97.5%. The ROC curves together with their confidence
bands are represented in @fig-comparison_methods, separately for the three
density-generating processes. @tbl-comparison_auc summarises the performance of the methods across
the schemes, by means of the average area under the curve (AUC).

We can see that both ICS methods give quite similar results except for
the `GP_clr` generating process where `ICS_B2` outperforms `ICS_L2`.
Together with `MUOD_L2` and `MUOD_QF`, these methods are the best in
terms of average AUC, although ICS-based methods perform more
consistently across the different generating schemes. The `Median_LQD`
and `MBD_LQD` methods are worse than the others for all generating
schemes. Overall, we can recommend ICS versus the other outlier
detection methods in this situation where the proportion of outliers is
small.

```{r fig-comparison_methods}
#| fig-cap: ROC curves of 10 different outlier detection methods for
#|   density data with 3 generating schemes.
#| fig-height: 7
#| message: false
#| cache.lazy: false

methods <- c(
  "ICS_B2",
  "ICS_L2",
  "Median_LQD",
  "Median_Wasserstein",
  "MBD_L2",
  "MBD_LQD",
  "MBD_QF",
  "MUOD_L2",
  "MUOD_LQD",
  "MUOD_QF"
)
N <- 200 # Number of repetitions per combination

method_ics_b2 <- function(data) {
  u <- fda.usc::fdata(log(pmax(data, 1e-8)), argvals = as.numeric(colnames(data))) |>
    fda.usc::fdata2fd(nbasis = 10)
  for (i in integer(3)) {
    u$coefs <- sweep(u$coefs, 2, fda::inprod(u) / diff(u$basis$rangeval))
  }
  if (any(is.nan(u$coefs))) stop("ICS preprocessing has failed for at least one density")
  ICS_outlier(u, n_cores = 14)$ics_distances
}

method_ics_l2 <- function(data) {
  f <- fda.usc::fdata(data, argvals = as.numeric(colnames(data))) |>
    fda.usc::fdata2fd(nbasis = 10)
  ICS_outlier(f, n_cores = 14)$ics_distances
}

method_deboinr <- function(data, distance) {
  density_order <- deboinr(
    x_grid = seq(0, 1, length.out = ncol(data)),
    densities_matrix = data,
    distance = distance,
    num_cores = 14
  )$density_order
  order(density_order)
}

pdf_to_quant <- function(pdf, x_grid) {
  interp_fn <- approxfun(x_grid, pdf)
  cdf <- DeBoinR:::pdf_to_cdf(pdf, x_grid, norm = TRUE)
  quant <- DeBoinR:::cdf_to_quant(cdf, x_grid)
  return(quant)
}

pdf_transform <- function(data, transform = c("id", "QF", "LQD")) {
  switch(transform,
    "id" = data,
    "QF" = t(apply(
      data, 1,
      function(f) pdf_to_quant(f, seq(0, 1, length.out = ncol(data)))
    )),
    "LQD" = t(apply(
      data, 1,
      function(f) {
        DeBoinR:::pdf_to_lqd(
          DeBoinR:::alpha_mix(f, 0.1),
          seq(0, 1, length.out = ncol(data))
        )
      }
    ))
  )
}

method_muod <- function(data) {
  res <- muod(data)$indices
  res <- apply(res, 2, function(x) (x - quantile(x, 0.75)) / IQR(x))
  apply(res, 1, max)
}

outlier_score <- function(method, data) {
  print(paste("Computing outlier scores with", method))
  tryCatch(switch(method,
    "ICS_B2" = method_ics_b2(data),
    "ICS_L2" = method_ics_l2(data),
    "Median_LQD" = method_deboinr(data, distance = "nLQD"),
    "Median_Wasserstein" = method_deboinr(data, distance = "wasserstein"),
    "MBD_L2" = -modified_band_depth(data),
    "MBD_LQD" = -modified_band_depth(pdf_transform(data, "LQD")),
    "MBD_QF" = -modified_band_depth(pdf_transform(data, "QF")),
    "MUOD_L2" = method_muod(data),
    "MUOD_LQD" = method_muod(pdf_transform(data, "LQD")),
    "MUOD_QF" = method_muod(pdf_transform(data, "QF"))
  ), error = function(e) NA)
}

compute_roc <- function(scores, true_outliers) {
  thresholds <- c(scores, max(scores) + 1)
  tibble(threshold = thresholds) |>
    mutate(
      TP = map_int(threshold, ~ sum(scores >= .x & true_outliers)),
      FP = map_int(threshold, ~ sum(scores >= .x & !true_outliers)),
      FN = map_int(threshold, ~ sum(scores < .x & true_outliers)),
      TN = map_int(threshold, ~ sum(scores < .x & !true_outliers)),
      PP = map_int(threshold, ~ sum(scores >= .x)),
      TPR = TP / (TP + FN),
      FPR = FP / (FP + TN),
    ) |>
    select(TPR, FPR, PP)
}

compute_auc_2 <- function(scores, true_outliers) {
  thresholds <- scores[which(true_outliers)]
  P <- sum(true_outliers)
  N <- sum(!true_outliers)
  tibble(threshold = thresholds) |>
    mutate(
      is_ordered = map_int(threshold, ~ sum(scores < .x & !true_outliers)) / (P * N),
    ) |>
    pull(is_ordered) |>
    sum()
}

if (!(file.exists("data/ICS_comparison.RData")) & !(file.exists("data/ICS_comparison_data.RData"))) {
  comparison_data <- crossing(Simulation = 1:N, Scheme = schemes) |>
    mutate(sim = map(Scheme, generate_data)) |>
    crossing(Method = methods)
  save(comparison_data, file = "data/ICS_comparison_data.RData")
}

if (!(file.exists("data/ICS_comparison.RData"))) {
  load("data/ICS_comparison_data.RData")
  comparison <- comparison_data |>
    filter(Method %in% methods & Scheme %in% schemes) |>
    unnest_wider(sim) |>
    mutate(
      scores = map2(Method, data, outlier_score),
      roc = map2(scores, true_outliers, compute_roc)
    ) |>
    unnest(roc) |>
    select(!data)
  save(comparison, file = "data/ICS_comparison.RData")
}

load("data/ICS_comparison.RData")

# Aggregate results (averaging over simulations)
comparison_avg <- comparison |>
  separate_wider_delim(Method, "_", names = c("Approach", "Metric")) |>
  group_by(Scheme, Metric, Approach, PP) |>
  summarize(
    n = n(),
    TPR_avg = mean(TPR),
    TPR_conf = qnorm(0.975) * sd(TPR) / sqrt(n()),
    FPR_avg = mean(FPR),
    FPR_conf = qnorm(0.975) * sd(FPR) / sqrt(n()),
    .groups = "drop"
  )

ggplot(comparison_avg, aes(FPR_avg, TPR_avg, color = Scheme, group = Scheme)) +
  geom_line() +
  geom_ribbon(aes(
    ymin = pmax(0, TPR_avg - TPR_conf),
    ymax = pmin(1, TPR_avg + TPR_conf),
    xmin = pmax(0, FPR_avg - FPR_conf),
    xmax = pmin(1, FPR_avg + FPR_conf),
    fill = Scheme
  ), alpha = 0.1, linetype = "dotted") +
  geom_function(fun = identity, linetype = "dotted") +
  facet_wrap(vars(paste(Approach, Metric, sep = "_")), nrow = 2) +
  coord_fixed() +
  labs(
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)",
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r tbl-comparison_auc}
#| tbl-cap: AUC for the 10 outlier detection methods, averaged across the 3
#|   generating schemes.

comparison_auc <- comparison_avg |>
  group_by(Scheme, Approach, Metric) |>
  summarise(AUC = integrate(approxfun(FPR_avg, TPR_avg), 0, 1)$value)

comparison_auc |>
  group_by(Approach, Metric) |>
  summarise(`Average AUC` = mean(AUC)) |>
  arrange(desc(`Average AUC`)) |>
  mutate(across(where(is.numeric), ~ signif(.x, digits = 2)))
```
